{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a6349e",
   "metadata": {},
   "source": [
    "# MIDS W205 Fall 2021 Project 3\n",
    "### Instructor: Shiraz Chakraverty\n",
    "### Student: Ben Mok\n",
    "### Team Members: Aastha Khanna, Ben Mok, Don Irwin, Theresa Kuruvilla\n",
    "### Section Tuesday 6 P.M.\n",
    "#### Date: 12/02/2021\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "In this document we will explain the key components of project 3.\n",
    "\n",
    "We will inspect the environment we set up.\n",
    "\n",
    "We will inspect the code we run create the pipeline from and push data through it.\n",
    "\n",
    "The Video link below gives a walk-through of our entire project end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1183cb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://www.youtube.com/watch?v=TpS3rIrctBo\" target=\"https://www.youtube.com/watch?v=TpS3rIrctBo\"> Click on this text or the image below to view an explanation video</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<a href=\"https://www.youtube.com/watch?v=TpS3rIrctBo\" target=\"https://www.youtube.com/watch?v=TpS3rIrctBo\"> Click on this text or the image below to view an explanation video</a>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1dbfdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://www.youtube.com/watch?v=TpS3rIrctBo\" target=\"https://youtu.be/Mgce9pA9ASc\"> <img src=\"https://tuneman7.github.io/video.png\" border=0, width=\"20%\">    </a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<a href=\"https://www.youtube.com/watch?v=TpS3rIrctBo\" target=\"https://youtu.be/Mgce9pA9ASc\"> <img src=\"https://tuneman7.github.io/video.png\" border=0, width=\"20%\">    </a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aed93f",
   "metadata": {},
   "source": [
    "***\n",
    "## Team Contribution To the Codebase:\n",
    "\n",
    "The instructor has been granted read access, and may view check-in history on our collaboration github repo, to view team member checkins for verification of the assessment below:\n",
    "\n",
    "https://github.com/mids-w205-chakraverty/project_3_team_2_tue_6_30\n",
    "\n",
    "Aastha:  \n",
    "\n",
    "Contributed to hive table extraction and supported general development.  Was also responsible for finding fix which saved many hours.  Assisted Theresa in applying this fix to her environment.\n",
    "\n",
    "Ben:  \n",
    "\n",
    "Created Parameterized random sythetic event pitcher, YML file as well as, demonstrated understanding of the entire pipeline end-to-end. \n",
    "\n",
    "Theresa: \n",
    "\n",
    "Contributed to Flask API endpoint.\n",
    "\n",
    "Don: \n",
    "\n",
    "Solution architect, contributed to steaming, query, table schema, and automation.\n",
    "\n",
    "Lise:  \n",
    "\n",
    "Dropped the class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974bc40a",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Building blocks of the pipeline\n",
    "\n",
    "1. The YML file:\n",
    "\n",
    "    This is infrastructure as code file.  It specifies all the containers and their connections to one another.\n",
    "\n",
    "2.  Configuraton Files:\n",
    "\n",
    "    Some configuration files \"log4j.properties\" and the like are used for configuring spark.\n",
    "\n",
    "3. Fask API file:\n",
    "\n",
    "    These python files which we submit through Spark-Submit, this makes the pipeline reproductable and extensible.\n",
    "\n",
    "4. Hive Table Definition Files\n",
    "\n",
    "5. Spark Submit Program\n",
    "\n",
    "6. Synthetic Parameterized Event Generation\n",
    "\n",
    "7. Hive Query Files.\n",
    "\n",
    "8. Jupyter Notebook File For Hive Reporting.\n",
    "\n",
    "9. Bash Script File.\n",
    "    \n",
    "\n",
    "We will walk through each of these building blocks one by one.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b5b29",
   "metadata": {},
   "source": [
    "### The YML File -- Our Infrastructure as Code Environment:\n",
    "\n",
    "#### The YML file can be found at the following location:\n",
    "./code_files/docker-compose.yml\n",
    "\n",
    "Let us inspect the YML file, please read comments below:\n",
    "\n",
    "```yml\n",
    "---\n",
    "#  Don Irwin 12/02/2021\n",
    "#  \n",
    "#  Some notable changes include exposing drives on certain containers.\n",
    "#  Additionally for the conflientinc images we are no longer using the \"latest\" images.\n",
    "#  This is because the latest images have changes in them which do not user \"--zookeeper\" flags\n",
    "#  for topic creations from the command prompt.\n",
    "#  \n",
    "#  This changed on us during the work.\n",
    "#  Hence we had to revert to versions :5.3.1 in order for our code to continue to work.\n",
    "#\n",
    "#\n",
    "version: '2'\n",
    "services:\n",
    "  redis:\n",
    "    image: redis:latest\n",
    "    expose:\n",
    "      - \"6379\"\n",
    "      \n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:5.3.1\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:5.3.1\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  cloudera:\n",
    "    image: midsw205/hadoop:0.0.2\n",
    "    hostname: cloudera\n",
    "    expose:\n",
    "      - \"8020\" # nn\n",
    "      - \"8888\" # hue\n",
    "      - \"9083\" # hive thrift\n",
    "      - \"10000\" # hive jdbc\n",
    "      - \"50070\" # nn http\n",
    "    ports:\n",
    "      - \"8888:8888\"\n",
    "      - \"9093\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "    volumes:\n",
    "      - ~/w205:/w205      \n",
    "\n",
    "  spark:\n",
    "    image: midsw205/spark-python:0.0.6\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    expose:\n",
    "      - \"8888\"\n",
    "    #ports:\n",
    "    #  - \"8888:8888\"\n",
    "    depends_on:\n",
    "      - cloudera\n",
    "    environment:\n",
    "      HADOOP_NAMENODE: cloudera\n",
    "      HIVE_THRIFTSERVER: cloudera:9083\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "    command: bash\n",
    "\n",
    "  presto:\n",
    "    image: midsw205/presto:0.0.1\n",
    "    hostname: presto\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    expose:\n",
    "      - \"8080\"\n",
    "    ports:\n",
    "      - \"8082:8080\" # Adding binding to local port 8082 for connection from notebooks; 8080 was in use\n",
    "    environment:\n",
    "      HIVE_THRIFTSERVER: cloudera:9083\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  mids:\n",
    "    image: midsw205/base:0.1.9\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    expose:\n",
    "      - \"5000\"\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "    command: jupyter notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293b655",
   "metadata": {},
   "source": [
    "### Configuration Files\n",
    "\n",
    "####  The Spark log4j.properties file is available at the following location.\n",
    "\n",
    "./code_files/log4j.properties\n",
    "\n",
    "We are interested in this file because adjusting a setting shuts down Pyspark's verbose warning messaging.\n",
    "\n",
    "The line we are interested in is below.\n",
    "\n",
    "```bash\n",
    "\n",
    "log4j.rootCategory=FATAL, console\n",
    "\n",
    "```\n",
    "\n",
    "We copy our modified log4j.properties file to our spark instance using the following command.\n",
    "\n",
    "```bash\n",
    "\n",
    "docker-compose exec spark bash -c \"cp /w205/project-3-tuneman7/code_files/log4j.properties ./conf/log4j.properties\"\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9614dbd3",
   "metadata": {},
   "source": [
    "### Flask API File:\n",
    "\n",
    "####  The Python Query file is available at the following location:\n",
    "\n",
    "./code_files/game_api.py\n",
    "\n",
    "This file contains the Python code run within the flask http server.\n",
    "\n",
    "```bash\n",
    "\n",
    "docker-compose exec mids env FLASK_APP=/w205/project-3-tuneman7/code_files/game_api.py flask run >> log_file1.txt &\n",
    "\n",
    "```\n",
    "\n",
    "Let us inspect this file's contents:\n",
    "\n",
    "```python\n",
    "\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import json\n",
    "import uuid\n",
    "from kafka import KafkaProducer\n",
    "from flask import Flask, request, session\n",
    "from flask import jsonify\n",
    "import sys\n",
    "from multiprocessing import Value\n",
    "\n",
    "\n",
    "counter = Value('i', 0)\n",
    "app = Flask(__name__)\n",
    "\n",
    "event_id = 0\n",
    "\n",
    "def get_event_id():\n",
    "    out = str(uuid.uuid4())\n",
    "    return out\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)\n",
    "\n",
    "app = Flask(__name__)\n",
    "producer = KafkaProducer(bootstrap_servers='kafka:29092')\n",
    "\n",
    "def log_event_parameters():\n",
    "    args = request.args\n",
    "    print(args, file=sys.stderr)\n",
    "    print(request.args.to_dict(),file=sys.stderr)\n",
    "    print(request.args.viewkeys(),file=sys.stderr)\n",
    "    key_views = request.args.viewkeys()\n",
    "    output_list = []\n",
    "    output_dict = {}\n",
    "    event_id = get_event_id()\n",
    "    return_string = \"\"\n",
    "    key_count = 0\n",
    "    for key in request.args.viewkeys():\n",
    "        my_dict = {}\n",
    "        my_dict[\"event_id\"] = event_id\n",
    "        my_dict[\"parameter_name\"] = key\n",
    "        my_dict[\"parameter_value\"] = request.args.get(key)       \n",
    "        print(key,file=sys.stderr)\n",
    "        print(request.args.get(key),file=sys.stderr)\n",
    "        output_dict[key]=request.args.get(key)       \n",
    "        output_list.append(my_dict)\n",
    "        key_count = key_count +1\n",
    "        producer.send(\"event_parameters\", json.dumps(my_dict).encode())\n",
    "    if key_count==0:\n",
    "        my_dict = {}\n",
    "        my_dict[\"event_id\"] = event_id\n",
    "        my_dict[\"parameter_name\"] = \"user\"\n",
    "        my_dict[\"parameter_value\"] = \"NONE\"\n",
    "        producer.send(\"event_parameters\", json.dumps(my_dict).encode())\n",
    "    return event_id\n",
    "\n",
    "def log_to_kafka(topic, event):\n",
    "    event_id = log_event_parameters()\n",
    "    event.update(request.headers)\n",
    "    event_id_dict={'event_id':event_id}\n",
    "    event.update(event_id_dict)\n",
    "    producer.send(topic, json.dumps(event).encode())\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def default_response():\n",
    "    default_event = {'event_type': 'default'}\n",
    "    log_to_kafka('events', default_event)\n",
    "    return \"This is the default response!\\n\"\n",
    "\n",
    "\n",
    "@app.route(\"/purchase_a_sword\")\n",
    "def purchase_a_sword():\n",
    "    purchase_sword_event = {'event_type': 'purchase_sword'}\n",
    "    log_to_kafka('events', purchase_sword_event)\n",
    "    return \"Sword Purchased!\\n\"\n",
    "\n",
    "@app.route(\"/join_a_guild\")\n",
    "def join_guild():\n",
    "    join_guild_event = {'event_type': 'join_guild'}\n",
    "    log_to_kafka('events', join_guild_event)\n",
    "    return \"Joined Guild!\\n\"\n",
    "\n",
    "@app.route(\"/leave_guild\")\n",
    "def leave_guild():\n",
    "    leave_guild_event = {'event_type': 'leave_guild'}\n",
    "    log_to_kafka('events', leave_guild_event)\n",
    "    return \"Left Guild!\\n\"\n",
    "\n",
    "@app.route(\"/get_credit\")\n",
    "def get_credit():\n",
    "    get_credit_event = {'event_type': 'get_credit'}\n",
    "    log_to_kafka('events', get_credit_event)\n",
    "    return \"Received Credit!\\n\"\n",
    "\n",
    "@app.route(\"/shutdown\")\n",
    "def shutdown():\n",
    "    func = request.environ.get('werkzeug.server.shutdown')\n",
    "    if func is None:\n",
    "        raise RuntimeError('Not running with the Werkzeug Server')\n",
    "    func()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560f55f",
   "metadata": {},
   "source": [
    "***\n",
    "### Hive Table Hive Table Definition Files\n",
    "\n",
    "####  The HQL file can be found at the following location:\n",
    "\n",
    "./code_files/hive_table_creation.hql\n",
    "\n",
    "This file is executed within our pipeline in the following way:\n",
    "\n",
    "```bash\n",
    "docker-compose exec cloudera hive -f /w205/project-3-tuneman7/code_files/hive_table_creation.hql \n",
    "```\n",
    "Its contents are below:\n",
    "\n",
    "```sql\n",
    "\n",
    "create external table if not exists default.all_events (\n",
    "    raw_event string,\n",
    "    timestamp string,\n",
    "    Accept string,\n",
    "    Host string,\n",
    "    User_Agent string,\n",
    "    event_id string,\n",
    "    event_type string\n",
    "  )\n",
    "  stored as parquet \n",
    "  location '/tmp/all_events'\n",
    "  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "  \n",
    "\n",
    "create external table if not exists default.event_parameters (\n",
    "    raw_event string,\n",
    "    timestamp string,\n",
    "    Accept string,\n",
    "    Host string,\n",
    "    User_Agent string,\n",
    "    event_id string,\n",
    "    parameter_name string,\n",
    "    parameter_value string\n",
    "    \n",
    "  )\n",
    "  stored as parquet \n",
    "  location '/tmp/event_parameters'\n",
    "  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581a5f4",
   "metadata": {},
   "source": [
    "***\n",
    "### Spark Submit Program\n",
    "\n",
    "####  The Spark Submit program can be found at the following location:\n",
    "\n",
    "./code_files/separate_event_stream_2.py\n",
    "\n",
    "This file is executed within our pipeline in the following way:\n",
    "\n",
    "```bash\n",
    "\n",
    "docker-compose exec spark spark-submit /w205/project-3-tuneman7/code_files/separate_events_stream_2.py &\n",
    "\n",
    "```\n",
    "Its contents are below:\n",
    "\n",
    "```python\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Extract events from kafka and write them to hdfs\n",
    "\"\"\"\n",
    "import json,time\n",
    "from pyspark.sql import SparkSession, Row\n",
    "#from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def general_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_id\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "def event_parameter_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_id\", StringType(), True),\n",
    "        StructField(\"parameter_name\", StringType(), True),\n",
    "        StructField(\"parameter_value\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@udf('string')\n",
    "def munge_event(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    event['Host'] = \"moe\"\n",
    "    event['Cache-Control'] = \"no-cache\"\n",
    "    return json.dumps(event)\n",
    "\n",
    "def main():\n",
    "    \"\"\"main\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ExtractEventsJob\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    raw_events = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .load()\n",
    "    \n",
    "    raw_event_parameters = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"event_parameters\") \\\n",
    "        .load()\n",
    "\n",
    "    event_parameters = raw_event_parameters \\\n",
    "        .select(raw_event_parameters.value.cast('string').alias('raw_event'),\n",
    "                raw_event_parameters.timestamp.cast('string'),\n",
    "                from_json(raw_event_parameters.value.cast('string'),\n",
    "                          event_parameter_event_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "    all_events = raw_events \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          general_event_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')    \n",
    "    \n",
    "    sink1 = event_parameters \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_event_parameters\") \\\n",
    "        .option(\"path\", \"/tmp/event_parameters\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "        \n",
    "    sink2 = all_events \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_all_events\") \\\n",
    "        .option(\"path\", \"/tmp/all_events\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()        \n",
    "    \n",
    "\n",
    "    sink1.awaitTermination()\n",
    "    sink2.awaitTermination()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56866b75",
   "metadata": {},
   "source": [
    "***\n",
    "### Sythetic Parameterized Event (data) Generation\n",
    "\n",
    "####  This program can be found at the following location:\n",
    "\n",
    "./code_files/primative_event_pitcher_ab_2.py\n",
    "\n",
    "This file is executed within our pipeline in the following way:\n",
    "\n",
    "```bash\n",
    "\n",
    "python primative_event_pitcher_ab_2.py >> log_event_pitcher.txt\n",
    "\n",
    "```\n",
    "\n",
    "Its contents are listed below:\n",
    "\n",
    "```python\n",
    "\n",
    "import sys, getopt,os,smtplib,time\n",
    "from os.path import basename\n",
    "from email.mime.application import MIMEApplication\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.utils import COMMASPACE, formatdate\n",
    "import subprocess\n",
    "import random\n",
    "\n",
    "def main():\n",
    "    \n",
    "    with open('guild_names.csv', encoding='ISO-8859-1') as g:\n",
    "        guild_names = [row.split(',')[0].strip('\\n') for row in g]\n",
    "        g.close()\n",
    "    with open('sword_types.csv',encoding='ISO-8859-1') as s:\n",
    "        sword_types = [row.split(',')[0].strip('\\n') for row in s]\n",
    "        s.close()\n",
    "    rand_int  = random.randint(5,110)\n",
    "    thisdir   = os.getcwd()\n",
    "    users     = ['ben', 'aastha', 'lise', 'theresa', 'don']\n",
    "    events    = ['purchase_a_sword', 'join_a_guild', 'leave_guild', 'get_credit']\n",
    "    \n",
    "    \n",
    "    flask_shutdown_command = \"docker-compose exec mids curl http://localhost:5000/shutdown\"\n",
    "\n",
    "    print(\"pitching events\")\n",
    "    # Just create 10000 events.\n",
    "    i = 1\n",
    "    \n",
    "    while i < 140:\n",
    "        i+=1\n",
    "        e_randint = random.randint(0,len(events)-1)\n",
    "        u_randint = random.randint(0,len(users)-1)\n",
    "        g_randint = random.randint(0,len(guild_names)-1)\n",
    "        s_randint = random.randint(0,len(sword_types)-1)\n",
    "        b_randint = random.randint(0,40)\n",
    "        base      = 'docker-compose exec mids ab -n {} -H \"Host: user2.att.com\" http://localhost:5000/'.format(b_randint)\n",
    "\n",
    "        \n",
    "        if events[e_randint] =='join_a_guild':\n",
    "            line = base + 'join_a_guild\"?user={}&guild_name={}\"'\n",
    "            line = line.format(users[u_randint],guild_names[g_randint])\n",
    "        elif events[e_randint] =='leave_guild':\n",
    "            line = base + 'leave_guild\"?user={}&guild_name={}\"'\n",
    "            line = line.format(users[u_randint],guild_names[g_randint])\n",
    "        elif events[e_randint] == 'purchase_a_sword':\n",
    "            line = base + 'purchase_a_sword\"?user={}&sword_type={}\"'\n",
    "            line = line.format(users[u_randint],sword_types[s_randint])\n",
    "        else:\n",
    "            line = base + 'get_credit\"?user={}&guild_name={}\"'\n",
    "            line = line.format(users[u_randint],guild_names[g_randint])\n",
    "        print(line)\n",
    "        subprocess.call(line, shell=True)\n",
    "        print(\"Press and HOLD CTRL+C to terminate, else 10000 events will be created\")\n",
    "        \n",
    "        \n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa78fa",
   "metadata": {},
   "source": [
    "***\n",
    "### Hive Query File\n",
    "\n",
    "This is a HQL file which contains \"canned\" queries to answer questions about the state of the game / pipeline.\n",
    "\n",
    "####  This program can be found at the following location:\n",
    "\n",
    "./code_files/query_hive_tables.hql\n",
    "\n",
    "This file is executed within our pipeline in the following way:\n",
    "\n",
    "```bash\n",
    "\n",
    "docker-compose exec presto presto --server presto:8080 --catalog hive --schema default -f /w205/project-3-tuneman7/code_files/query_hive_tables.hql \n",
    "\n",
    "```\n",
    "\n",
    "Its contents are listed below:\n",
    "\n",
    "```sql\n",
    "\n",
    "select '-----------LOOK AT TOTAL EVENT COUNTS--------------------';\n",
    "select\n",
    "    event_type,\n",
    "    count(event_type) as event_count\n",
    "from \n",
    "    all_events\n",
    "group by event_type;\n",
    "select '-----------LOOK USER GUILD JOIN COUNT--------------------';\n",
    "select \n",
    "    un.parameter_value as user_name,\n",
    "    et.event_type as event,\n",
    "    count(un.parameter_value) as guild_join_count\n",
    "from \n",
    "    all_events et\n",
    "join \n",
    "    event_parameters un\n",
    "on \n",
    "    et.event_id = un.event_id\n",
    "and \n",
    "    et.event_type = 'join_guild'\n",
    "and \n",
    "    un.parameter_name = 'user'    \n",
    "group by \n",
    "    un.parameter_value\n",
    "    ,et.event_type\n",
    "order by \n",
    "    count(un.parameter_value) desc limit 10;\n",
    "select '-----------LOOK USER GUILD LEAVE COUNT--------------------';\n",
    "select \n",
    "    un.parameter_value as user_name,\n",
    "    et.event_type as event,\n",
    "    count(un.parameter_value) as guild_leave_count\n",
    "from \n",
    "    all_events et\n",
    "join \n",
    "    event_parameters un\n",
    "on \n",
    "    et.event_id = un.event_id\n",
    "and \n",
    "    et.event_type = 'leave_guild'\n",
    "and \n",
    "    un.parameter_name = 'user'    \n",
    "group by \n",
    "    un.parameter_value\n",
    "    ,et.event_type\n",
    "order by \n",
    "    count(un.parameter_value) desc limit 10;\n",
    "select '-----------LOOK USER GET CREDIT COUNT--------------------';\n",
    "select \n",
    "    un.parameter_value as user_name,\n",
    "    et.event_type as event,\n",
    "    count(un.parameter_value) as get_credit_count\n",
    "from \n",
    "    all_events et\n",
    "join \n",
    "    event_parameters un\n",
    "on \n",
    "    et.event_id = un.event_id\n",
    "and \n",
    "    et.event_type = 'get_credit'\n",
    "and \n",
    "    un.parameter_name = 'user'    \n",
    "group by \n",
    "    un.parameter_value\n",
    "    ,et.event_type\n",
    "order by \n",
    "    count(un.parameter_value) desc limit 10;\n",
    "select '-----------LOOK AT 10 MOST POPULAR SWORDS--------------------';\n",
    "select \n",
    "    un.parameter_value as sword_name,\n",
    "    et.event_type as event,\n",
    "    count(un.parameter_value) as popular_sword_count\n",
    "from \n",
    "    all_events et\n",
    "join \n",
    "    event_parameters un\n",
    "on \n",
    "    et.event_id = un.event_id\n",
    "and \n",
    "    un.parameter_name = 'sword_type'\n",
    "group by \n",
    "    un.parameter_value\n",
    "    ,et.event_type\n",
    "order by \n",
    "    count(un.parameter_value) desc limit 10;\n",
    "select '-----------LOOK AT 10 MOST POPULAR GUILDS--------------------';\n",
    "select \n",
    "    un.parameter_value as guild_name,\n",
    "    et.event_type as event,\n",
    "    count(un.parameter_value) as popular_guild_count\n",
    "from \n",
    "    all_events et\n",
    "join \n",
    "    event_parameters un\n",
    "on \n",
    "    et.event_id = un.event_id\n",
    "and \n",
    "    un.parameter_name = 'guild_name'\n",
    "and \n",
    "    et.event_type = 'join_guild'\n",
    "group by \n",
    "    un.parameter_value\n",
    "    ,et.event_type\n",
    "order by \n",
    "    count(un.parameter_value) desc limit 10;\n",
    "select '-------------------------------';    \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc8576",
   "metadata": {},
   "source": [
    "***\n",
    "### Jyputer Notebook for Hive Reporting\n",
    "\n",
    "This is a jupyter notebook which contains basic queries and answers to questions.\n",
    "\n",
    "####  This file can be found at the following location:\n",
    "\n",
    "./code_files/hive_reports.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbcc178",
   "metadata": {},
   "source": [
    "***\n",
    "### Bash Script for Running the Pipeline\n",
    "\n",
    "This is the \"glue\" program that runs the pipeline end-to-end\n",
    "\n",
    "####  This file can be found at the following location:\n",
    "\n",
    "./code_files/dd.sh\n",
    "\n",
    "This program is executed in the following way:\n",
    "\n",
    "```bash\n",
    ". dd.sh\n",
    "```\n",
    "\n",
    "The contents of this file are below:\n",
    "\n",
    "```bash\n",
    "\n",
    "#mkdir ~/w205/spark-from-files/\n",
    "#cd ~/w205/spark-from-files\n",
    "#cp ~/w205/course-content/11-Storing-Data-III/docker-compose.yml .\n",
    "# cp ~/w205/course-content/11-Storing-Data-III/*.py .\n",
    "#bring up images\n",
    "docker-compose up -d\n",
    "echo \"sleeping 30\"\n",
    "sleep 30\n",
    "echo \"looking at HDFS\"\n",
    "\n",
    "#look at hdf\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "\n",
    "echo \"creating topic\"\n",
    "\n",
    "docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "sleep 3\n",
    "docker-compose exec kafka kafka-topics --create --topic event_parameters --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "echo \"sleeping\"\n",
    "sleep 4\n",
    "rm log_file1.txt\n",
    "echo \" \">log_file1.txt\n",
    "\n",
    "#spin up API endpoint flask container.\n",
    "docker-compose exec mids env FLASK_APP=/w205/project-3-tuneman7/code_files/game_api.py flask run >> log_file1.txt &\n",
    "echo \"sleeping\"\n",
    "sleep 4\n",
    "echo \"copying config files over\"\n",
    "echo \"docker-compose exec spark bash -c \\\"cp /w205/project-3-tuneman7/code_files/log4j.properties ./conf/log4j.properties\\\"\"\n",
    "docker-compose exec spark bash -c \"cp /w205/project-3-tuneman7/code_files/log4j.properties ./conf/log4j.properties\"\n",
    "\n",
    "#run spark submit\n",
    "echo \"doing the spark submit\"\n",
    "echo \"docker-compose exec spark spark-submit /w205/project-3-tuneman7/code_files/separate_events_stream_2.py\"\n",
    "docker-compose exec spark spark-submit /w205/project-3-tuneman7/code_files/separate_events_stream_2.py &\n",
    "\n",
    "#create hive tables\n",
    "echo \"docker-compose exec cloudera hive -f /w205/project-3-tuneman7/code_files/hive_table_creation.hql\"\n",
    "docker-compose exec cloudera hive -f /w205/project-3-tuneman7/code_files/hive_table_creation.hql \n",
    "\n",
    "#look at hive tables and checkpoint tables.\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "\n",
    "\n",
    "#Run synthetic parameterized event generation.\n",
    "echo \"python primative_event_pitcher_ab_2.py > log_event_pitcher.txt\"\n",
    "echo \"press and HOLD CTL+C to terminate:\"\n",
    "x=1\n",
    "while [ $x -le 500 ]\n",
    "do\n",
    "  #Run the event pitcher\n",
    "  python primative_event_pitcher_ab_2.py >> log_event_pitcher.txt\n",
    "  #Cycle flask\n",
    "  docker-compose exec mids curl http://localhost:5000/shutdown\n",
    "  docker-compose exec mids env FLASK_APP=/w205/project-3-tuneman7/code_files/game_api.py flask run >> log_file1.txt &\n",
    "  #Run the queries and see results\n",
    "  docker-compose exec presto presto --server presto:8080 --catalog hive --schema default -f /w205/project-3-tuneman7/code_files/query_hive_tables.hql \n",
    "  sleep 2\n",
    "  echo \"press and HOLD CTL+C to terminate:\"\n",
    "done\n",
    "\n",
    "#docker-compose down\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7219ca34",
   "metadata": {},
   "source": [
    "### Running the pipeline end-to-end (single cycle)\n",
    "\n",
    "####  A single-cycle pipeline (one that does only one batch of apache-bench calls) is created below.\n",
    "\n",
    "```bash\n",
    "\n",
    ". run_demo_of_pipeline.sh\n",
    "\n",
    "```\n",
    "\n",
    "It is not possible to run this pipeline within this notebook.  However, once it is run its output can be gotten.\n",
    "\n",
    "Below are some ad-hoq query results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f5871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyhive in /opt/conda/lib/python3.7/site-packages (0.6.4)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyhive) (0.18.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from pyhive) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil->pyhive) (1.16.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for jupyter-contrib-nbextensions: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/site-packages/jupyter_contrib_nbextensions-0.5.1.dist-info/METADATA'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyhive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9becb996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>event_parameters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Table\n",
       "0        all_events\n",
       "1  event_parameters"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyhive import presto\n",
    "import pandas as pd\n",
    "\n",
    "presto_conn = presto.connect(\n",
    "    host='0.0.0.0',\n",
    "    port=8082 # Exposed Presto port (see docker compose file)\n",
    ")\n",
    "\n",
    "pd.read_sql_query(\"SHOW TABLES\", presto_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3a0f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>event</th>\n",
       "      <th>guild_join_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lise</td>\n",
       "      <td>join_guild</td>\n",
       "      <td>2380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>don</td>\n",
       "      <td>join_guild</td>\n",
       "      <td>2053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ben</td>\n",
       "      <td>join_guild</td>\n",
       "      <td>1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aastha</td>\n",
       "      <td>join_guild</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theresa</td>\n",
       "      <td>join_guild</td>\n",
       "      <td>1761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_name       event  guild_join_count\n",
       "0      lise  join_guild              2380\n",
       "1       don  join_guild              2053\n",
       "2       ben  join_guild              1863\n",
       "3    aastha  join_guild              1800\n",
       "4   theresa  join_guild              1761"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_parameters = pd.read_sql_query(\"select     un.parameter_value as user_name,     et.event_type as event,     count(un.parameter_value) as guild_join_count  from      all_events et  join      event_parameters un on     et.event_id = un.event_id and    et.event_type = 'join_guild' and     un.parameter_name = 'user'     group by     un.parameter_value    ,et.event_type order by     count(un.parameter_value) desc limit 10\", presto_conn)\n",
    "event_parameters.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5471cf23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sword_name</th>\n",
       "      <th>event</th>\n",
       "      <th>popular_sword_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sacrifice</td>\n",
       "      <td>purchase_sword</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Metallium Slither</td>\n",
       "      <td>purchase_sword</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Justice</td>\n",
       "      <td>purchase_sword</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unhappy Ending</td>\n",
       "      <td>purchase_sword</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Innocence End</td>\n",
       "      <td>purchase_sword</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sword_name           event  popular_sword_count\n",
       "0              Sacrifice  purchase_sword                  225\n",
       "1  The Metallium Slither  purchase_sword                  222\n",
       "2                Justice  purchase_sword                  191\n",
       "3         Unhappy Ending  purchase_sword                  189\n",
       "4          Innocence End  purchase_sword                  176"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_parameters = pd.read_sql_query(\"select     un.parameter_value as sword_name,    et.event_type as event,    count(un.parameter_value) as popular_sword_count from     all_events et join     event_parameters un on     et.event_id = un.event_id and     un.parameter_name = 'sword_type' group by     un.parameter_value     ,et.event_type order by     count(un.parameter_value) desc limit 10\", presto_conn)\n",
    "event_parameters.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6529dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
